{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Import Libraries & Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using Apple MPS (GPU) ðŸš€\n",
            "TRAIN_DIR exists: True\n",
            "TEST_DIR  exists: True\n",
            "Setup complete!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import copy\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms, datasets\n",
        "from tqdm import tqdm\n",
        "\n",
        "# â”€â”€ Reproducibility â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# â”€â”€ Device: Apple Silicon MPS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device('mps')\n",
        "    print('Using Apple MPS (GPU) ðŸš€')\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    print(f'Using CUDA: {torch.cuda.get_device_name(0)}')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    print('Using CPU')\n",
        "\n",
        "# â”€â”€ Paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "BASE_DIR  = './data'\n",
        "TRAIN_DIR = f'{BASE_DIR}/train'\n",
        "TEST_DIR  = f'{BASE_DIR}/kaggle_test_final'\n",
        "\n",
        "print(f'TRAIN_DIR exists: {os.path.exists(TRAIN_DIR)}')\n",
        "print(f'TEST_DIR  exists: {os.path.exists(TEST_DIR)}')\n",
        "\n",
        "# â”€â”€ Hyperparameters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "IMG_SIZE   = 224\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS     = 15\n",
        "LR         = 0.001\n",
        "VAL_SPLIT  = 0.15\n",
        "PATIENCE   = 5\n",
        "\n",
        "print('Setup complete!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Data Transforms (Train / Val / TTA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transforms ready!\n"
          ]
        }
      ],
      "source": [
        "MEAN = [0.485, 0.456, 0.406]\n",
        "STD  = [0.229, 0.224, 0.225]\n",
        "\n",
        "# â”€â”€ Train: Strong augmentation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE + 32, IMG_SIZE + 32)),\n",
        "    transforms.RandomCrop(IMG_SIZE),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.05),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=MEAN, std=STD),\n",
        "    transforms.RandomErasing(p=0.2, scale=(0.02, 0.2))\n",
        "])\n",
        "\n",
        "# â”€â”€ Val: No augmentation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=MEAN, std=STD)\n",
        "])\n",
        "\n",
        "# â”€â”€ TTA: 5 versions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "tta_transforms_list = [\n",
        "    transforms.Compose([  # 1) Original\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=MEAN, std=STD)\n",
        "    ]),\n",
        "    transforms.Compose([  # 2) Horizontal flip\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.RandomHorizontalFlip(p=1.0),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=MEAN, std=STD)\n",
        "    ]),\n",
        "    transforms.Compose([  # 3) Center crop\n",
        "        transforms.Resize((IMG_SIZE + 32, IMG_SIZE + 32)),\n",
        "        transforms.CenterCrop(IMG_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=MEAN, std=STD)\n",
        "    ]),\n",
        "    transforms.Compose([  # 4) Rotation +10\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.RandomRotation((10, 10)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=MEAN, std=STD)\n",
        "    ]),\n",
        "    transforms.Compose([  # 5) Rotation -10\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.RandomRotation((-10, -10)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=MEAN, std=STD)\n",
        "    ]),\n",
        "]\n",
        "\n",
        "print('Transforms ready!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Load Dataset & Train/Val Split (85/15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classes found: ['chihuahua', 'muffin']\n",
            "class_to_idx : {'chihuahua': 0, 'muffin': 1}\n",
            "Total images : 4733\n",
            "Train: 4024 | Val: 709\n"
          ]
        }
      ],
      "source": [
        "# â”€â”€ Load full training dataset â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "full_dataset = datasets.ImageFolder(root=TRAIN_DIR, transform=train_transforms)\n",
        "classes      = full_dataset.classes\n",
        "idx_to_class = {v: k for k, v in full_dataset.class_to_idx.items()}\n",
        "print(f'Classes found: {classes}')\n",
        "print(f'class_to_idx : {full_dataset.class_to_idx}')\n",
        "print(f'Total images : {len(full_dataset)}')\n",
        "\n",
        "# â”€â”€ Train / Val split 85/15 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "val_size   = int(len(full_dataset) * VAL_SPLIT)\n",
        "train_size = len(full_dataset) - val_size\n",
        "train_subset, val_subset = random_split(\n",
        "    full_dataset,\n",
        "    [train_size, val_size],\n",
        "    generator=torch.Generator().manual_seed(SEED)\n",
        ")\n",
        "\n",
        "val_dataset_clean           = copy.deepcopy(val_subset.dataset)\n",
        "val_dataset_clean.transform = val_transforms\n",
        "val_dataset_final           = torch.utils.data.Subset(val_dataset_clean, val_subset.indices)\n",
        "\n",
        "# â”€â”€ DataLoaders â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# num_workers=0 for Apple MPS compatibility (avoid multiprocessing issues) --- IGNORE ---\n",
        "train_loader = DataLoader(train_subset,      batch_size=BATCH_SIZE, shuffle=True,  num_workers=0)\n",
        "val_loader   = DataLoader(val_dataset_final, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "print(f'Train: {train_size} | Val: {val_size}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Test Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test images: 1138\n"
          ]
        }
      ],
      "source": [
        "class TestDataset(Dataset):\n",
        "    def __init__(self, test_dir, transform=None):\n",
        "        self.test_dir    = test_dir\n",
        "        self.transform   = transform\n",
        "        self.image_files = sorted([\n",
        "            f for f in os.listdir(test_dir)\n",
        "            if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_files[idx]\n",
        "        img_path = os.path.join(self.test_dir, img_name)\n",
        "        image    = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, img_name\n",
        "\n",
        "print(f'Test images: {len(TestDataset(TEST_DIR))}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Build Model â€” Custom CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model ready! Params: 1,240,482 / 1,240,482\n"
          ]
        }
      ],
      "source": [
        "class MuffinChihuahuaCNN(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(MuffinChihuahuaCNN, self).__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            # Block 1: 224 â†’ 112\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32), nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32), nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), nn.Dropout2d(0.1),\n",
        "\n",
        "            # Block 2: 112 â†’ 56\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64), nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64), nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), nn.Dropout2d(0.15),\n",
        "\n",
        "            # Block 3: 56 â†’ 28\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128), nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128), nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), nn.Dropout2d(0.2),\n",
        "\n",
        "            # Block 4: 28 â†’ 14\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256), nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256), nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), nn.Dropout2d(0.2),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256, 256), nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "model     = MuffinChihuahuaCNN(num_classes=2).to(device)\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total     = sum(p.numel() for p in model.parameters())\n",
        "print(f'Model ready! Params: {trainable:,} / {total:,}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Phase 1 â€” Train All Layers (LR=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== Phase 1: Training (All Layers) =====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [00:53<00:00,  2.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [01/15] Train Loss: 0.5809 Acc: 73.51% | Val Loss: 0.4464 Acc: 86.60% â˜… BEST\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [00:47<00:00,  2.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [02/15] Train Loss: 0.4874 Acc: 80.54% | Val Loss: 0.3811 Acc: 87.73% â˜… BEST\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [00:47<00:00,  2.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [03/15] Train Loss: 0.4728 Acc: 82.70% | Val Loss: 0.4534 Acc: 83.07%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [00:46<00:00,  2.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [04/15] Train Loss: 0.4590 Acc: 84.62% | Val Loss: 0.3933 Acc: 87.45%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [00:45<00:00,  2.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [05/15] Train Loss: 0.4548 Acc: 83.47% | Val Loss: 0.4079 Acc: 87.45%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [00:45<00:00,  2.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [06/15] Train Loss: 0.4458 Acc: 84.84% | Val Loss: 0.3788 Acc: 88.72% â˜… BEST\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:11<00:00,  1.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [07/15] Train Loss: 0.4304 Acc: 85.98% | Val Loss: 0.3849 Acc: 88.72%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [00:46<00:00,  2.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [08/15] Train Loss: 0.4248 Acc: 85.88% | Val Loss: 0.3538 Acc: 90.41% â˜… BEST\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [00:47<00:00,  2.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [09/15] Train Loss: 0.4282 Acc: 86.01% | Val Loss: 0.3538 Acc: 89.84%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [00:46<00:00,  2.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [10/15] Train Loss: 0.4173 Acc: 86.08% | Val Loss: 0.3481 Acc: 90.55% â˜… BEST\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [00:46<00:00,  2.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [11/15] Train Loss: 0.4006 Acc: 88.00% | Val Loss: 0.3475 Acc: 90.13%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [00:46<00:00,  2.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [12/15] Train Loss: 0.4048 Acc: 87.57% | Val Loss: 0.3629 Acc: 88.72%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [00:46<00:00,  2.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [13/15] Train Loss: 0.3888 Acc: 88.10% | Val Loss: 0.3467 Acc: 89.84%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [00:46<00:00,  2.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [14/15] Train Loss: 0.3915 Acc: 88.37% | Val Loss: 0.3488 Acc: 89.70%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [00:46<00:00,  2.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [15/15] Train Loss: 0.3838 Acc: 89.07% | Val Loss: 0.3409 Acc: 90.69% â˜… BEST\n",
            "\n",
            "Best Val Accuracy (Phase 1): 90.69%\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-6)\n",
        "\n",
        "best_val_acc = 0.0\n",
        "best_weights = copy.deepcopy(model.state_dict())\n",
        "no_improve   = 0\n",
        "\n",
        "print('===== Phase 1: Training (All Layers) =====')\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    # â”€â”€ Train â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    model.train()\n",
        "    running_loss, correct, total_n = 0.0, 0, 0\n",
        "    for images, labels in tqdm(train_loader, desc=f'Epoch {epoch}/{EPOCHS} [Train]'):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss    = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct  += (preds == labels).sum().item()\n",
        "        total_n  += labels.size(0)\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_acc  = 100 * correct / total_n\n",
        "\n",
        "    # â”€â”€ Validate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    model.eval()\n",
        "    val_loss_sum, val_correct, val_total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs       = model(images)\n",
        "            loss          = criterion(outputs, labels)\n",
        "            val_loss_sum += loss.item()\n",
        "            _, preds      = torch.max(outputs, 1)\n",
        "            val_correct  += (preds == labels).sum().item()\n",
        "            val_total    += labels.size(0)\n",
        "    val_loss = val_loss_sum / len(val_loader)\n",
        "    val_acc  = 100 * val_correct / val_total\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    flag = ' â˜… BEST' if val_acc > best_val_acc else ''\n",
        "    print(f'Epoch [{epoch:02d}/{EPOCHS}] '\n",
        "          f'Train Loss: {train_loss:.4f} Acc: {train_acc:.2f}% | '\n",
        "          f'Val Loss: {val_loss:.4f} Acc: {val_acc:.2f}%{flag}')\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_weights = copy.deepcopy(model.state_dict())\n",
        "        # à¸šà¸±à¸™à¸—à¸¶à¸ checkpoint à¹„à¸§à¹‰à¸”à¹‰à¸§à¸¢ à¸à¸±à¸™à¹€à¸„à¸£à¸·à¹ˆà¸­à¸‡ crash\n",
        "        torch.save(best_weights, 'best_model.pth')\n",
        "        no_improve   = 0\n",
        "    else:\n",
        "        no_improve += 1\n",
        "        if no_improve >= PATIENCE:\n",
        "            print(f'Early stopping at epoch {epoch}')\n",
        "            break\n",
        "\n",
        "print(f'\\nBest Val Accuracy (Phase 1): {best_val_acc:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Phase 2 â€” Fine-tune All Layers (LR=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Phase 2 Trainable params: 1,240,482\n",
            "\n",
            "===== Phase 2: Fine-tuning (Lower LR) =====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 [Fine-tune]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [00:46<00:00,  2.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [01/10] Train Loss: 0.3942 Acc: 88.07% | Val Loss: 0.3559 Acc: 89.84%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Fine-tune]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [00:46<00:00,  2.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [02/10] Train Loss: 0.3846 Acc: 88.89% | Val Loss: 0.3325 Acc: 91.96% â˜… BEST\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Fine-tune]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [00:46<00:00,  2.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [03/10] Train Loss: 0.3885 Acc: 88.54% | Val Loss: 0.3345 Acc: 90.97%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Fine-tune]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [00:46<00:00,  2.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [04/10] Train Loss: 0.3866 Acc: 88.62% | Val Loss: 0.3456 Acc: 90.13%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 [Fine-tune]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [00:46<00:00,  2.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [05/10] Train Loss: 0.3865 Acc: 88.87% | Val Loss: 0.3277 Acc: 91.96%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/10 [Fine-tune]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [00:46<00:00,  2.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [06/10] Train Loss: 0.3828 Acc: 88.44% | Val Loss: 0.3530 Acc: 89.84%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/10 [Fine-tune]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [00:46<00:00,  2.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [07/10] Train Loss: 0.3840 Acc: 88.99% | Val Loss: 0.3450 Acc: 90.27%\n",
            "Early stopping at epoch 7\n",
            "\n",
            "Final Best Val Accuracy: 91.96%\n"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(best_weights)\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Phase 2 Trainable params: {trainable:,}')\n",
        "\n",
        "optimizer2 = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "EPOCHS2    = 10\n",
        "scheduler2 = optim.lr_scheduler.CosineAnnealingLR(optimizer2, T_max=EPOCHS2, eta_min=1e-6)\n",
        "no_improve = 0\n",
        "\n",
        "print('\\n===== Phase 2: Fine-tuning (Lower LR) =====')\n",
        "\n",
        "for epoch in range(1, EPOCHS2 + 1):\n",
        "    # â”€â”€ Train â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    model.train()\n",
        "    running_loss, correct, total_n = 0.0, 0, 0\n",
        "    for images, labels in tqdm(train_loader, desc=f'Epoch {epoch}/{EPOCHS2} [Fine-tune]'):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer2.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss    = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer2.step()\n",
        "        running_loss += loss.item()\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct  += (preds == labels).sum().item()\n",
        "        total_n  += labels.size(0)\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_acc  = 100 * correct / total_n\n",
        "\n",
        "    # â”€â”€ Validate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    model.eval()\n",
        "    val_loss_sum, val_correct, val_total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs       = model(images)\n",
        "            loss          = criterion(outputs, labels)\n",
        "            val_loss_sum += loss.item()\n",
        "            _, preds      = torch.max(outputs, 1)\n",
        "            val_correct  += (preds == labels).sum().item()\n",
        "            val_total    += labels.size(0)\n",
        "    val_loss = val_loss_sum / len(val_loader)\n",
        "    val_acc  = 100 * val_correct / val_total\n",
        "\n",
        "    scheduler2.step()\n",
        "\n",
        "    flag = ' â˜… BEST' if val_acc > best_val_acc else ''\n",
        "    print(f'Epoch [{epoch:02d}/{EPOCHS2}] '\n",
        "          f'Train Loss: {train_loss:.4f} Acc: {train_acc:.2f}% | '\n",
        "          f'Val Loss: {val_loss:.4f} Acc: {val_acc:.2f}%{flag}')\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_weights = copy.deepcopy(model.state_dict())\n",
        "        torch.save(best_weights, 'best_model.pth')\n",
        "        no_improve   = 0\n",
        "    else:\n",
        "        no_improve += 1\n",
        "        if no_improve >= PATIENCE:\n",
        "            print(f'Early stopping at epoch {epoch}')\n",
        "            break\n",
        "\n",
        "model.load_state_dict(best_weights)\n",
        "print(f'\\nFinal Best Val Accuracy: {best_val_acc:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: TTA Inference on Test Set (5 passes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting TTA Inference...\n",
            "  TTA pass 1/5...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                      \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  TTA pass 2/5...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                      \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  TTA pass 3/5...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                      \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  TTA pass 4/5...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                      \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  TTA pass 5/5...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                      "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done! Total predictions: 1138\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "print('Starting TTA Inference...')\n",
        "model.eval()\n",
        "softmax = nn.Softmax(dim=1)\n",
        "\n",
        "all_probs     = None\n",
        "all_filenames = None\n",
        "\n",
        "for t_idx, tta_transform in enumerate(tta_transforms_list):\n",
        "    print(f'  TTA pass {t_idx + 1}/{len(tta_transforms_list)}...')\n",
        "    test_dataset = TestDataset(test_dir=TEST_DIR, transform=tta_transform)\n",
        "    test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=False, num_workers=0)\n",
        "    pass_probs = []\n",
        "    pass_files = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, filenames in tqdm(test_loader, desc=f'TTA {t_idx+1}', leave=False):\n",
        "            images  = images.to(device)\n",
        "            outputs = model(images)\n",
        "            probs   = softmax(outputs).cpu().numpy()\n",
        "            pass_probs.append(probs)\n",
        "            if t_idx == 0:\n",
        "                pass_files.extend(filenames)\n",
        "\n",
        "    pass_probs = np.concatenate(pass_probs, axis=0)\n",
        "    if all_probs is None:\n",
        "        all_probs     = pass_probs\n",
        "        all_filenames = pass_files\n",
        "    else:\n",
        "        all_probs += pass_probs\n",
        "\n",
        "all_probs   /= len(tta_transforms_list)\n",
        "pred_indices = np.argmax(all_probs, axis=1)\n",
        "predictions  = [idx_to_class[i] for i in pred_indices]\n",
        "\n",
        "print(f'Done! Total predictions: {len(predictions)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Save submission.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "submission.csv saved!\n",
            "               ID    Predict\n",
            "0     img_0_0.jpg     muffin\n",
            "1    img_0_10.jpg     muffin\n",
            "2  img_0_1000.jpg     muffin\n",
            "3  img_0_1037.jpg     muffin\n",
            "4   img_0_105.jpg     muffin\n",
            "5  img_0_1050.jpg     muffin\n",
            "6  img_0_1051.jpg     muffin\n",
            "7  img_0_1061.jpg     muffin\n",
            "8  img_0_1063.jpg     muffin\n",
            "9  img_0_1071.jpg  chihuahua\n",
            "\n",
            "Prediction distribution:\n",
            "Predict\n",
            "chihuahua    643\n",
            "muffin       495\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "submission_df = pd.DataFrame({\n",
        "    'ID'     : all_filenames,\n",
        "    'Predict': predictions\n",
        "})\n",
        "\n",
        "submission_df.to_csv('submission.csv', index=False)\n",
        "print('submission.csv saved!')\n",
        "print(submission_df.head(10))\n",
        "print(f'\\nPrediction distribution:')\n",
        "print(submission_df['Predict'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Confusion Matrix & Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Classification Report (Validation Set) ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   chihuahua       0.93      0.91      0.92       364\n",
            "      muffin       0.91      0.92      0.92       345\n",
            "\n",
            "    accuracy                           0.92       709\n",
            "   macro avg       0.92      0.92      0.92       709\n",
            "weighted avg       0.92      0.92      0.92       709\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x500 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "model.eval()\n",
        "y_true, y_pred_all = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        images   = images.to(device)\n",
        "        outputs  = model(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred_all.extend(preds.cpu().numpy())\n",
        "\n",
        "print('--- Classification Report (Validation Set) ---')\n",
        "print(classification_report(y_true, y_pred_all, target_names=classes))\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred_all)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=classes, yticklabels=classes)\n",
        "plt.title(f'Confusion Matrix â€” Val Acc: {best_val_acc:.2f}%')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
